{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lr7rB3szzhtx"
      },
      "source": [
        "## Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "giM74oK1rRIH"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Obtaining file:///local/data/hs3447/llama3/LLaMA-Factory\n",
            "  Installing build dependencies ... \u001b[?25ldone\n",
            "\u001b[?25h  Checking if build backend supports build_editable ... \u001b[?25ldone\n",
            "\u001b[?25h  Getting requirements to build editable ... \u001b[?25ldone\n",
            "\u001b[?25h  Preparing editable metadata (pyproject.toml) ... \u001b[?25ldone\n",
            "\u001b[?25hCollecting transformers>=4.41.2 (from llamafactory==0.8.3.dev0)\n",
            "  Using cached transformers-4.42.4-py3-none-any.whl.metadata (43 kB)\n",
            "Collecting datasets>=2.16.0 (from llamafactory==0.8.3.dev0)\n",
            "  Using cached datasets-2.20.0-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting accelerate>=0.30.1 (from llamafactory==0.8.3.dev0)\n",
            "  Using cached accelerate-0.32.1-py3-none-any.whl.metadata (18 kB)\n",
            "Collecting peft>=0.11.1 (from llamafactory==0.8.3.dev0)\n",
            "  Using cached peft-0.11.1-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting trl>=0.8.6 (from llamafactory==0.8.3.dev0)\n",
            "  Using cached trl-0.9.6-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting gradio>=4.0.0 (from llamafactory==0.8.3.dev0)\n",
            "  Using cached gradio-4.38.1-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting pandas>=2.0.0 (from llamafactory==0.8.3.dev0)\n",
            "  Using cached pandas-2.2.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (19 kB)\n",
            "Collecting scipy (from llamafactory==0.8.3.dev0)\n",
            "  Using cached scipy-1.14.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "Collecting einops (from llamafactory==0.8.3.dev0)\n",
            "  Using cached einops-0.8.0-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting sentencepiece (from llamafactory==0.8.3.dev0)\n",
            "  Using cached sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
            "Collecting tiktoken (from llamafactory==0.8.3.dev0)\n",
            "  Using cached tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Collecting protobuf (from llamafactory==0.8.3.dev0)\n",
            "  Using cached protobuf-5.27.2-cp38-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)\n",
            "Collecting uvicorn (from llamafactory==0.8.3.dev0)\n",
            "  Using cached uvicorn-0.30.1-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting pydantic (from llamafactory==0.8.3.dev0)\n",
            "  Using cached pydantic-2.8.2-py3-none-any.whl.metadata (125 kB)\n",
            "Collecting fastapi (from llamafactory==0.8.3.dev0)\n",
            "  Using cached fastapi-0.111.1-py3-none-any.whl.metadata (26 kB)\n",
            "Collecting sse-starlette (from llamafactory==0.8.3.dev0)\n",
            "  Using cached sse_starlette-2.1.2-py3-none-any.whl.metadata (5.8 kB)\n",
            "Collecting matplotlib>=3.7.0 (from llamafactory==0.8.3.dev0)\n",
            "  Using cached matplotlib-3.9.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Collecting fire (from llamafactory==0.8.3.dev0)\n",
            "  Using cached fire-0.6.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: packaging in /local/data/hs3447/anaconda3/envs/factory/lib/python3.10/site-packages (from llamafactory==0.8.3.dev0) (24.1)\n",
            "Collecting pyyaml (from llamafactory==0.8.3.dev0)\n",
            "  Using cached PyYAML-6.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)\n",
            "Collecting numpy<2.0.0 (from llamafactory==0.8.3.dev0)\n",
            "  Using cached numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "Collecting torch>=1.13.1 (from llamafactory==0.8.3.dev0)\n",
            "  Using cached torch-2.3.1-cp310-cp310-manylinux1_x86_64.whl.metadata (26 kB)\n",
            "Collecting bitsandbytes>=0.39.0 (from llamafactory==0.8.3.dev0)\n",
            "  Using cached bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl.metadata (2.2 kB)\n",
            "Requirement already satisfied: psutil in /local/data/hs3447/anaconda3/envs/factory/lib/python3.10/site-packages (from accelerate>=0.30.1->llamafactory==0.8.3.dev0) (6.0.0)\n",
            "Collecting huggingface-hub (from accelerate>=0.30.1->llamafactory==0.8.3.dev0)\n",
            "  Using cached huggingface_hub-0.23.5-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting safetensors>=0.3.1 (from accelerate>=0.30.1->llamafactory==0.8.3.dev0)\n",
            "  Using cached safetensors-0.4.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
            "Collecting filelock (from datasets>=2.16.0->llamafactory==0.8.3.dev0)\n",
            "  Using cached filelock-3.15.4-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting pyarrow>=15.0.0 (from datasets>=2.16.0->llamafactory==0.8.3.dev0)\n",
            "  Downloading pyarrow-17.0.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
            "Collecting pyarrow-hotfix (from datasets>=2.16.0->llamafactory==0.8.3.dev0)\n",
            "  Using cached pyarrow_hotfix-0.6-py3-none-any.whl.metadata (3.6 kB)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets>=2.16.0->llamafactory==0.8.3.dev0)\n",
            "  Using cached dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting requests>=2.32.2 (from datasets>=2.16.0->llamafactory==0.8.3.dev0)\n",
            "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting tqdm>=4.66.3 (from datasets>=2.16.0->llamafactory==0.8.3.dev0)\n",
            "  Using cached tqdm-4.66.4-py3-none-any.whl.metadata (57 kB)\n",
            "Collecting xxhash (from datasets>=2.16.0->llamafactory==0.8.3.dev0)\n",
            "  Using cached xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess (from datasets>=2.16.0->llamafactory==0.8.3.dev0)\n",
            "  Using cached multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.5.0,>=2023.1.0 (from fsspec[http]<=2024.5.0,>=2023.1.0->datasets>=2.16.0->llamafactory==0.8.3.dev0)\n",
            "  Using cached fsspec-2024.5.0-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting aiohttp (from datasets>=2.16.0->llamafactory==0.8.3.dev0)\n",
            "  Using cached aiohttp-3.9.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.5 kB)\n",
            "Collecting aiofiles<24.0,>=22.0 (from gradio>=4.0.0->llamafactory==0.8.3.dev0)\n",
            "  Using cached aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting altair<6.0,>=5.0 (from gradio>=4.0.0->llamafactory==0.8.3.dev0)\n",
            "  Using cached altair-5.3.0-py3-none-any.whl.metadata (9.2 kB)\n",
            "Collecting ffmpy (from gradio>=4.0.0->llamafactory==0.8.3.dev0)\n",
            "  Using cached ffmpy-0.3.2-py3-none-any.whl\n",
            "Collecting gradio-client==1.1.0 (from gradio>=4.0.0->llamafactory==0.8.3.dev0)\n",
            "  Using cached gradio_client-1.1.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting httpx>=0.24.1 (from gradio>=4.0.0->llamafactory==0.8.3.dev0)\n",
            "  Using cached httpx-0.27.0-py3-none-any.whl.metadata (7.2 kB)\n",
            "Collecting importlib-resources<7.0,>=1.3 (from gradio>=4.0.0->llamafactory==0.8.3.dev0)\n",
            "  Using cached importlib_resources-6.4.0-py3-none-any.whl.metadata (3.9 kB)\n",
            "Collecting jinja2<4.0 (from gradio>=4.0.0->llamafactory==0.8.3.dev0)\n",
            "  Using cached jinja2-3.1.4-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting markupsafe~=2.0 (from gradio>=4.0.0->llamafactory==0.8.3.dev0)\n",
            "  Using cached MarkupSafe-2.1.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
            "Collecting orjson~=3.0 (from gradio>=4.0.0->llamafactory==0.8.3.dev0)\n",
            "  Using cached orjson-3.10.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (50 kB)\n",
            "Collecting pillow<11.0,>=8.0 (from gradio>=4.0.0->llamafactory==0.8.3.dev0)\n",
            "  Using cached pillow-10.4.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (9.2 kB)\n",
            "Collecting pydub (from gradio>=4.0.0->llamafactory==0.8.3.dev0)\n",
            "  Using cached pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting python-multipart>=0.0.9 (from gradio>=4.0.0->llamafactory==0.8.3.dev0)\n",
            "  Using cached python_multipart-0.0.9-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting ruff>=0.2.2 (from gradio>=4.0.0->llamafactory==0.8.3.dev0)\n",
            "  Using cached ruff-0.5.2-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (24 kB)\n",
            "Collecting semantic-version~=2.0 (from gradio>=4.0.0->llamafactory==0.8.3.dev0)\n",
            "  Using cached semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting tomlkit==0.12.0 (from gradio>=4.0.0->llamafactory==0.8.3.dev0)\n",
            "  Using cached tomlkit-0.12.0-py3-none-any.whl.metadata (2.7 kB)\n",
            "Collecting typer<1.0,>=0.12 (from gradio>=4.0.0->llamafactory==0.8.3.dev0)\n",
            "  Using cached typer-0.12.3-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /local/data/hs3447/anaconda3/envs/factory/lib/python3.10/site-packages (from gradio>=4.0.0->llamafactory==0.8.3.dev0) (4.12.2)\n",
            "Collecting urllib3~=2.0 (from gradio>=4.0.0->llamafactory==0.8.3.dev0)\n",
            "  Using cached urllib3-2.2.2-py3-none-any.whl.metadata (6.4 kB)\n",
            "Collecting websockets<12.0,>=10.0 (from gradio-client==1.1.0->gradio>=4.0.0->llamafactory==0.8.3.dev0)\n",
            "  Using cached websockets-11.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Collecting contourpy>=1.0.1 (from matplotlib>=3.7.0->llamafactory==0.8.3.dev0)\n",
            "  Using cached contourpy-1.2.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.8 kB)\n",
            "Collecting cycler>=0.10 (from matplotlib>=3.7.0->llamafactory==0.8.3.dev0)\n",
            "  Using cached cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting fonttools>=4.22.0 (from matplotlib>=3.7.0->llamafactory==0.8.3.dev0)\n",
            "  Using cached fonttools-4.53.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (162 kB)\n",
            "Collecting kiwisolver>=1.3.1 (from matplotlib>=3.7.0->llamafactory==0.8.3.dev0)\n",
            "  Using cached kiwisolver-1.4.5-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (6.4 kB)\n",
            "Collecting pyparsing>=2.3.1 (from matplotlib>=3.7.0->llamafactory==0.8.3.dev0)\n",
            "  Using cached pyparsing-3.1.2-py3-none-any.whl.metadata (5.1 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /local/data/hs3447/anaconda3/envs/factory/lib/python3.10/site-packages (from matplotlib>=3.7.0->llamafactory==0.8.3.dev0) (2.9.0)\n",
            "Collecting pytz>=2020.1 (from pandas>=2.0.0->llamafactory==0.8.3.dev0)\n",
            "  Using cached pytz-2024.1-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Collecting tzdata>=2022.7 (from pandas>=2.0.0->llamafactory==0.8.3.dev0)\n",
            "  Using cached tzdata-2024.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting annotated-types>=0.4.0 (from pydantic->llamafactory==0.8.3.dev0)\n",
            "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting pydantic-core==2.20.1 (from pydantic->llamafactory==0.8.3.dev0)\n",
            "  Using cached pydantic_core-2.20.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Collecting sympy (from torch>=1.13.1->llamafactory==0.8.3.dev0)\n",
            "  Using cached sympy-1.13.0-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting networkx (from torch>=1.13.1->llamafactory==0.8.3.dev0)\n",
            "  Using cached networkx-3.3-py3-none-any.whl.metadata (5.1 kB)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.13.1->llamafactory==0.8.3.dev0)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.13.1->llamafactory==0.8.3.dev0)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.13.1->llamafactory==0.8.3.dev0)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.13.1->llamafactory==0.8.3.dev0)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.13.1->llamafactory==0.8.3.dev0)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.13.1->llamafactory==0.8.3.dev0)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.13.1->llamafactory==0.8.3.dev0)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.13.1->llamafactory==0.8.3.dev0)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.13.1->llamafactory==0.8.3.dev0)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch>=1.13.1->llamafactory==0.8.3.dev0)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.13.1->llamafactory==0.8.3.dev0)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting triton==2.3.1 (from torch>=1.13.1->llamafactory==0.8.3.dev0)\n",
            "  Using cached triton-2.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.13.1->llamafactory==0.8.3.dev0)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting regex!=2019.12.17 (from transformers>=4.41.2->llamafactory==0.8.3.dev0)\n",
            "  Using cached regex-2024.5.15-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
            "Collecting tokenizers<0.20,>=0.19 (from transformers>=4.41.2->llamafactory==0.8.3.dev0)\n",
            "  Using cached tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Collecting tyro>=0.5.11 (from trl>=0.8.6->llamafactory==0.8.3.dev0)\n",
            "  Using cached tyro-0.8.5-py3-none-any.whl.metadata (8.2 kB)\n",
            "Collecting click>=7.0 (from uvicorn->llamafactory==0.8.3.dev0)\n",
            "  Using cached click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting h11>=0.8 (from uvicorn->llamafactory==0.8.3.dev0)\n",
            "  Using cached h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Collecting starlette<0.38.0,>=0.37.2 (from fastapi->llamafactory==0.8.3.dev0)\n",
            "  Using cached starlette-0.37.2-py3-none-any.whl.metadata (5.9 kB)\n",
            "Collecting fastapi-cli>=0.0.2 (from fastapi->llamafactory==0.8.3.dev0)\n",
            "  Using cached fastapi_cli-0.0.4-py3-none-any.whl.metadata (7.0 kB)\n",
            "Collecting email_validator>=2.0.0 (from fastapi->llamafactory==0.8.3.dev0)\n",
            "  Using cached email_validator-2.2.0-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: six in /local/data/hs3447/anaconda3/envs/factory/lib/python3.10/site-packages (from fire->llamafactory==0.8.3.dev0) (1.16.0)\n",
            "Collecting termcolor (from fire->llamafactory==0.8.3.dev0)\n",
            "  Using cached termcolor-2.4.0-py3-none-any.whl.metadata (6.1 kB)\n",
            "Collecting anyio (from sse-starlette->llamafactory==0.8.3.dev0)\n",
            "  Using cached anyio-4.4.0-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting jsonschema>=3.0 (from altair<6.0,>=5.0->gradio>=4.0.0->llamafactory==0.8.3.dev0)\n",
            "  Using cached jsonschema-4.23.0-py3-none-any.whl.metadata (7.9 kB)\n",
            "Collecting toolz (from altair<6.0,>=5.0->gradio>=4.0.0->llamafactory==0.8.3.dev0)\n",
            "  Using cached toolz-0.12.1-py3-none-any.whl.metadata (5.1 kB)\n",
            "Collecting dnspython>=2.0.0 (from email_validator>=2.0.0->fastapi->llamafactory==0.8.3.dev0)\n",
            "  Using cached dnspython-2.6.1-py3-none-any.whl.metadata (5.8 kB)\n",
            "Collecting idna>=2.0.0 (from email_validator>=2.0.0->fastapi->llamafactory==0.8.3.dev0)\n",
            "  Using cached idna-3.7-py3-none-any.whl.metadata (9.9 kB)\n",
            "Collecting aiosignal>=1.1.2 (from aiohttp->datasets>=2.16.0->llamafactory==0.8.3.dev0)\n",
            "  Using cached aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\n",
            "Collecting attrs>=17.3.0 (from aiohttp->datasets>=2.16.0->llamafactory==0.8.3.dev0)\n",
            "  Using cached attrs-23.2.0-py3-none-any.whl.metadata (9.5 kB)\n",
            "Collecting frozenlist>=1.1.1 (from aiohttp->datasets>=2.16.0->llamafactory==0.8.3.dev0)\n",
            "  Using cached frozenlist-1.4.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets>=2.16.0->llamafactory==0.8.3.dev0)\n",
            "  Using cached multidict-6.0.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
            "Collecting yarl<2.0,>=1.0 (from aiohttp->datasets>=2.16.0->llamafactory==0.8.3.dev0)\n",
            "  Using cached yarl-1.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (31 kB)\n",
            "Collecting async-timeout<5.0,>=4.0 (from aiohttp->datasets>=2.16.0->llamafactory==0.8.3.dev0)\n",
            "  Using cached async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting certifi (from httpx>=0.24.1->gradio>=4.0.0->llamafactory==0.8.3.dev0)\n",
            "  Using cached certifi-2024.7.4-py3-none-any.whl.metadata (2.2 kB)\n",
            "Collecting httpcore==1.* (from httpx>=0.24.1->gradio>=4.0.0->llamafactory==0.8.3.dev0)\n",
            "  Using cached httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting sniffio (from httpx>=0.24.1->gradio>=4.0.0->llamafactory==0.8.3.dev0)\n",
            "  Using cached sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
            "Collecting charset-normalizer<4,>=2 (from requests>=2.32.2->datasets>=2.16.0->llamafactory==0.8.3.dev0)\n",
            "  Using cached charset_normalizer-3.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (33 kB)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.2 in /local/data/hs3447/anaconda3/envs/factory/lib/python3.10/site-packages (from anyio->sse-starlette->llamafactory==0.8.3.dev0) (1.2.2)\n",
            "Collecting shellingham>=1.3.0 (from typer<1.0,>=0.12->gradio>=4.0.0->llamafactory==0.8.3.dev0)\n",
            "  Using cached shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
            "Collecting rich>=10.11.0 (from typer<1.0,>=0.12->gradio>=4.0.0->llamafactory==0.8.3.dev0)\n",
            "  Using cached rich-13.7.1-py3-none-any.whl.metadata (18 kB)\n",
            "Collecting docstring-parser>=0.16 (from tyro>=0.5.11->trl>=0.8.6->llamafactory==0.8.3.dev0)\n",
            "  Using cached docstring_parser-0.16-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting shtab>=1.5.6 (from tyro>=0.5.11->trl>=0.8.6->llamafactory==0.8.3.dev0)\n",
            "  Using cached shtab-1.7.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting httptools>=0.5.0 (from uvicorn[standard]>=0.12.0->fastapi->llamafactory==0.8.3.dev0)\n",
            "  Using cached httptools-0.6.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
            "Collecting python-dotenv>=0.13 (from uvicorn[standard]>=0.12.0->fastapi->llamafactory==0.8.3.dev0)\n",
            "  Using cached python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn[standard]>=0.12.0->fastapi->llamafactory==0.8.3.dev0)\n",
            "  Using cached uvloop-0.19.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.12.0->fastapi->llamafactory==0.8.3.dev0)\n",
            "  Using cached watchfiles-0.22.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Collecting mpmath<1.4,>=1.1.0 (from sympy->torch>=1.13.1->llamafactory==0.8.3.dev0)\n",
            "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
            "Collecting jsonschema-specifications>=2023.03.6 (from jsonschema>=3.0->altair<6.0,>=5.0->gradio>=4.0.0->llamafactory==0.8.3.dev0)\n",
            "  Using cached jsonschema_specifications-2023.12.1-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting referencing>=0.28.4 (from jsonschema>=3.0->altair<6.0,>=5.0->gradio>=4.0.0->llamafactory==0.8.3.dev0)\n",
            "  Using cached referencing-0.35.1-py3-none-any.whl.metadata (2.8 kB)\n",
            "Collecting rpds-py>=0.7.1 (from jsonschema>=3.0->altair<6.0,>=5.0->gradio>=4.0.0->llamafactory==0.8.3.dev0)\n",
            "  Using cached rpds_py-0.19.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
            "Collecting markdown-it-py>=2.2.0 (from rich>=10.11.0->typer<1.0,>=0.12->gradio>=4.0.0->llamafactory==0.8.3.dev0)\n",
            "  Using cached markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /local/data/hs3447/anaconda3/envs/factory/lib/python3.10/site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio>=4.0.0->llamafactory==0.8.3.dev0) (2.18.0)\n",
            "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio>=4.0.0->llamafactory==0.8.3.dev0)\n",
            "  Using cached mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
            "Using cached accelerate-0.32.1-py3-none-any.whl (314 kB)\n",
            "Using cached bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl (119.8 MB)\n",
            "Using cached datasets-2.20.0-py3-none-any.whl (547 kB)\n",
            "Using cached gradio-4.38.1-py3-none-any.whl (12.4 MB)\n",
            "Using cached gradio_client-1.1.0-py3-none-any.whl (318 kB)\n",
            "Using cached tomlkit-0.12.0-py3-none-any.whl (37 kB)\n",
            "Using cached matplotlib-3.9.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.3 MB)\n",
            "Using cached numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
            "Using cached pandas-2.2.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.0 MB)\n",
            "Using cached peft-0.11.1-py3-none-any.whl (251 kB)\n",
            "Using cached pydantic-2.8.2-py3-none-any.whl (423 kB)\n",
            "Using cached pydantic_core-2.20.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
            "Using cached PyYAML-6.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (705 kB)\n",
            "Using cached torch-2.3.1-cp310-cp310-manylinux1_x86_64.whl (779.1 MB)\n",
            "Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Using cached triton-2.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (168.1 MB)\n",
            "Using cached transformers-4.42.4-py3-none-any.whl (9.3 MB)\n",
            "Using cached trl-0.9.6-py3-none-any.whl (245 kB)\n",
            "Using cached uvicorn-0.30.1-py3-none-any.whl (62 kB)\n",
            "Using cached einops-0.8.0-py3-none-any.whl (43 kB)\n",
            "Using cached fastapi-0.111.1-py3-none-any.whl (92 kB)\n",
            "Using cached protobuf-5.27.2-cp38-abi3-manylinux2014_x86_64.whl (309 kB)\n",
            "Using cached scipy-1.14.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (41.1 MB)\n",
            "Using cached sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "Using cached sse_starlette-2.1.2-py3-none-any.whl (9.3 kB)\n",
            "Using cached tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "Using cached aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
            "Using cached altair-5.3.0-py3-none-any.whl (857 kB)\n",
            "Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
            "Using cached click-8.1.7-py3-none-any.whl (97 kB)\n",
            "Using cached contourpy-1.2.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (305 kB)\n",
            "Using cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
            "Using cached dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "Using cached email_validator-2.2.0-py3-none-any.whl (33 kB)\n",
            "Using cached fastapi_cli-0.0.4-py3-none-any.whl (9.5 kB)\n",
            "Using cached fonttools-4.53.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\n",
            "Using cached fsspec-2024.5.0-py3-none-any.whl (316 kB)\n",
            "Using cached aiohttp-3.9.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "Using cached h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "Using cached httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "Using cached httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "Using cached huggingface_hub-0.23.5-py3-none-any.whl (402 kB)\n",
            "Using cached importlib_resources-6.4.0-py3-none-any.whl (38 kB)\n",
            "Using cached jinja2-3.1.4-py3-none-any.whl (133 kB)\n",
            "Using cached kiwisolver-1.4.5-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.6 MB)\n",
            "Using cached MarkupSafe-2.1.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n",
            "Using cached orjson-3.10.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)\n",
            "Using cached pillow-10.4.0-cp310-cp310-manylinux_2_28_x86_64.whl (4.5 MB)\n",
            "Downloading pyarrow-17.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (39.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.9/39.9 MB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hUsing cached pyparsing-3.1.2-py3-none-any.whl (103 kB)\n",
            "Using cached python_multipart-0.0.9-py3-none-any.whl (22 kB)\n",
            "Using cached pytz-2024.1-py2.py3-none-any.whl (505 kB)\n",
            "Using cached regex-2024.5.15-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (775 kB)\n",
            "Using cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
            "Using cached ruff-0.5.2-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.1 MB)\n",
            "Using cached safetensors-0.4.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "Using cached semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Using cached starlette-0.37.2-py3-none-any.whl (71 kB)\n",
            "Using cached anyio-4.4.0-py3-none-any.whl (86 kB)\n",
            "Using cached tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
            "Using cached tqdm-4.66.4-py3-none-any.whl (78 kB)\n",
            "Using cached typer-0.12.3-py3-none-any.whl (47 kB)\n",
            "Using cached tyro-0.8.5-py3-none-any.whl (103 kB)\n",
            "Using cached tzdata-2024.1-py2.py3-none-any.whl (345 kB)\n",
            "Using cached urllib3-2.2.2-py3-none-any.whl (121 kB)\n",
            "Using cached filelock-3.15.4-py3-none-any.whl (16 kB)\n",
            "Using cached multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "Using cached networkx-3.3-py3-none-any.whl (1.7 MB)\n",
            "Using cached pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
            "Using cached pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Using cached sympy-1.13.0-py3-none-any.whl (6.2 MB)\n",
            "Using cached termcolor-2.4.0-py3-none-any.whl (7.7 kB)\n",
            "Using cached xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "Using cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Using cached async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
            "Using cached attrs-23.2.0-py3-none-any.whl (60 kB)\n",
            "Using cached certifi-2024.7.4-py3-none-any.whl (162 kB)\n",
            "Using cached charset_normalizer-3.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (142 kB)\n",
            "Using cached dnspython-2.6.1-py3-none-any.whl (307 kB)\n",
            "Using cached docstring_parser-0.16-py3-none-any.whl (36 kB)\n",
            "Using cached frozenlist-1.4.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (239 kB)\n",
            "Using cached httptools-0.6.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (341 kB)\n",
            "Using cached idna-3.7-py3-none-any.whl (66 kB)\n",
            "Using cached jsonschema-4.23.0-py3-none-any.whl (88 kB)\n",
            "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
            "Using cached multidict-6.0.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (124 kB)\n",
            "Using cached python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Using cached rich-13.7.1-py3-none-any.whl (240 kB)\n",
            "Using cached shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
            "Using cached shtab-1.7.1-py3-none-any.whl (14 kB)\n",
            "Using cached sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
            "Using cached uvloop-0.19.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n",
            "Using cached watchfiles-0.22.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "Using cached websockets-11.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n",
            "Using cached yarl-1.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (301 kB)\n",
            "Using cached nvidia_nvjitlink_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "Using cached toolz-0.12.1-py3-none-any.whl (56 kB)\n",
            "Using cached jsonschema_specifications-2023.12.1-py3-none-any.whl (18 kB)\n",
            "Using cached markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
            "Using cached referencing-0.35.1-py3-none-any.whl (26 kB)\n",
            "Using cached rpds_py-0.19.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (355 kB)\n",
            "Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
            "Checking if build backend supports build_editable ... \u001b[?25ldone\n",
            "\u001b[?25hBuilding wheels for collected packages: llamafactory\n",
            "  Building editable for llamafactory (pyproject.toml) ... \u001b[?25ldone\n",
            "\u001b[?25h  Created wheel for llamafactory: filename=llamafactory-0.8.3.dev0-0.editable-py3-none-any.whl size=20640 sha256=56150462c5b7f15831ca5afab54d7844fbd7b9ea8a6d67a39b7d2e1d870d520f\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-py7tt5ic/wheels/4d/ee/38/caccb00d6d447c0ad9e94d5ea1a63f700d873f14614a3df785\n",
            "Successfully built llamafactory\n",
            "Installing collected packages: sentencepiece, pytz, pydub, mpmath, ffmpy, xxhash, websockets, uvloop, urllib3, tzdata, tqdm, toolz, tomlkit, termcolor, sympy, sniffio, shtab, shellingham, semantic-version, safetensors, ruff, rpds-py, regex, pyyaml, python-multipart, python-dotenv, pyparsing, pydantic-core, pyarrow-hotfix, protobuf, pillow, orjson, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, networkx, multidict, mdurl, markupsafe, kiwisolver, importlib-resources, idna, httptools, h11, fsspec, frozenlist, fonttools, filelock, einops, docstring-parser, dnspython, dill, cycler, click, charset-normalizer, certifi, attrs, async-timeout, annotated-types, aiofiles, yarl, uvicorn, triton, scipy, requests, referencing, pydantic, pyarrow, pandas, nvidia-cusparse-cu12, nvidia-cudnn-cu12, multiprocess, markdown-it-py, jinja2, httpcore, fire, email_validator, contourpy, anyio, aiosignal, watchfiles, tiktoken, starlette, rich, nvidia-cusolver-cu12, matplotlib, jsonschema-specifications, huggingface-hub, httpx, aiohttp, tyro, typer, torch, tokenizers, sse-starlette, jsonschema, gradio-client, transformers, fastapi-cli, datasets, bitsandbytes, altair, accelerate, trl, peft, fastapi, gradio, llamafactory\n",
            "Successfully installed accelerate-0.32.1 aiofiles-23.2.1 aiohttp-3.9.5 aiosignal-1.3.1 altair-5.3.0 annotated-types-0.7.0 anyio-4.4.0 async-timeout-4.0.3 attrs-23.2.0 bitsandbytes-0.43.1 certifi-2024.7.4 charset-normalizer-3.3.2 click-8.1.7 contourpy-1.2.1 cycler-0.12.1 datasets-2.20.0 dill-0.3.8 dnspython-2.6.1 docstring-parser-0.16 einops-0.8.0 email_validator-2.2.0 fastapi-0.111.1 fastapi-cli-0.0.4 ffmpy-0.3.2 filelock-3.15.4 fire-0.6.0 fonttools-4.53.1 frozenlist-1.4.1 fsspec-2024.5.0 gradio-4.38.1 gradio-client-1.1.0 h11-0.14.0 httpcore-1.0.5 httptools-0.6.1 httpx-0.27.0 huggingface-hub-0.23.5 idna-3.7 importlib-resources-6.4.0 jinja2-3.1.4 jsonschema-4.23.0 jsonschema-specifications-2023.12.1 kiwisolver-1.4.5 llamafactory-0.8.3.dev0 markdown-it-py-3.0.0 markupsafe-2.1.5 matplotlib-3.9.1 mdurl-0.1.2 mpmath-1.3.0 multidict-6.0.5 multiprocess-0.70.16 networkx-3.3 numpy-1.26.4 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.82 nvidia-nvtx-cu12-12.1.105 orjson-3.10.6 pandas-2.2.2 peft-0.11.1 pillow-10.4.0 protobuf-5.27.2 pyarrow-17.0.0 pyarrow-hotfix-0.6 pydantic-2.8.2 pydantic-core-2.20.1 pydub-0.25.1 pyparsing-3.1.2 python-dotenv-1.0.1 python-multipart-0.0.9 pytz-2024.1 pyyaml-6.0.1 referencing-0.35.1 regex-2024.5.15 requests-2.32.3 rich-13.7.1 rpds-py-0.19.0 ruff-0.5.2 safetensors-0.4.3 scipy-1.14.0 semantic-version-2.10.0 sentencepiece-0.2.0 shellingham-1.5.4 shtab-1.7.1 sniffio-1.3.1 sse-starlette-2.1.2 starlette-0.37.2 sympy-1.13.0 termcolor-2.4.0 tiktoken-0.7.0 tokenizers-0.19.1 tomlkit-0.12.0 toolz-0.12.1 torch-2.3.1 tqdm-4.66.4 transformers-4.42.4 triton-2.3.1 trl-0.9.6 typer-0.12.3 tyro-0.8.5 tzdata-2024.1 urllib3-2.2.2 uvicorn-0.30.1 uvloop-0.19.0 watchfiles-0.22.0 websockets-11.0.3 xxhash-3.4.1 yarl-1.9.4\n"
          ]
        }
      ],
      "source": [
        "!pip install -e .[torch,bitsandbytes]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mFailed to start the Kernel. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "os.environ['HF_HOME'] = '/local/data/huggingface_cache'\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = '1'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H9RXn_YQnn9f"
      },
      "source": [
        "### Check GPU environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ZkN-ktlsnrdU"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "try:\n",
        "  assert torch.cuda.is_available() is True\n",
        "except AssertionError:\n",
        "  print(\"Please set up a GPU before using LLaMA Factory: https://medium.com/mlearning-ai/training-yolov4-on-google-colab-316f8fff99c6\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TeYs5Lz-QJYk"
      },
      "source": [
        "## Update Identity Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ap_fvMBsQHJc"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "NAME = \"Llama-3\"\n",
        "AUTHOR = \"LLaMA Factory\"\n",
        "\n",
        "with open(\"data/identity.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "  dataset = json.load(f)\n",
        "\n",
        "for sample in dataset:\n",
        "  sample[\"output\"] = sample[\"output\"].replace(\"{{\"+ \"name\" + \"}}\", NAME).replace(\"{{\"+ \"author\" + \"}}\", AUTHOR)\n",
        "\n",
        "with open(\"data/identity.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "  json.dump(dataset, f, indent=2, ensure_ascii=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2QiXcvdzzW3Y"
      },
      "source": [
        "## Fine-tune model via LLaMA Board"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "YLsdS6V5yUMy"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running on local URL:  http://0.0.0.0:7861\n",
            "Running on public URL: https://e1debf16f8fc12fbbc.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n",
            "[INFO|tokenization_utils_base.py:2161] 2024-07-17 17:14:22,257 >> loading file tokenizer.json from cache at /local/data/huggingface_cache/hub/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/e1945c40cd546c78e41f1151f4db032b271faeaa/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2161] 2024-07-17 17:14:22,257 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2161] 2024-07-17 17:14:22,257 >> loading file special_tokens_map.json from cache at /local/data/huggingface_cache/hub/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/e1945c40cd546c78e41f1151f4db032b271faeaa/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2161] 2024-07-17 17:14:22,257 >> loading file tokenizer_config.json from cache at /local/data/huggingface_cache/hub/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/e1945c40cd546c78e41f1151f4db032b271faeaa/tokenizer_config.json\n",
            "[WARNING|logging.py:313] 2024-07-17 17:14:22,524 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "07/17/2024 17:14:22 - INFO - llamafactory.data.template - Replace eos token: <|eot_id|>\n",
            "07/17/2024 17:14:22 - INFO - llamafactory.data.template - Add pad token: <|eot_id|>\n",
            "[INFO|configuration_utils.py:733] 2024-07-17 17:14:22,602 >> loading configuration file config.json from cache at /local/data/huggingface_cache/hub/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/e1945c40cd546c78e41f1151f4db032b271faeaa/config.json\n",
            "[INFO|configuration_utils.py:800] 2024-07-17 17:14:22,602 >> Model config LlamaConfig {\n",
            "  \"_name_or_path\": \"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
            "  \"architectures\": [\n",
            "    \"LlamaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"eos_token_id\": 128009,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 14336,\n",
            "  \"max_position_embeddings\": 8192,\n",
            "  \"mlp_bias\": false,\n",
            "  \"model_type\": \"llama\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"pretraining_tp\": 1,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 500000.0,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.42.4\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 128256\n",
            "}\n",
            "\n",
            "07/17/2024 17:14:22 - INFO - llamafactory.model.patcher - Using KV cache for faster generation.\n",
            "[INFO|modeling_utils.py:3556] 2024-07-17 17:14:22,622 >> loading weights file model.safetensors from cache at /local/data/huggingface_cache/hub/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/e1945c40cd546c78e41f1151f4db032b271faeaa/model.safetensors.index.json\n",
            "[INFO|modeling_utils.py:1531] 2024-07-17 17:14:22,623 >> Instantiating LlamaForCausalLM model under default dtype torch.bfloat16.\n",
            "[INFO|configuration_utils.py:1000] 2024-07-17 17:14:22,624 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"eos_token_id\": 128009\n",
            "}\n",
            "\n",
            "Loading checkpoint shards: 100%|██████████████████| 4/4 [00:06<00:00,  1.71s/it]\n",
            "[INFO|modeling_utils.py:4364] 2024-07-17 17:14:29,759 >> All model checkpoint weights were used when initializing LlamaForCausalLM.\n",
            "\n",
            "[INFO|modeling_utils.py:4372] 2024-07-17 17:14:29,759 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at meta-llama/Meta-Llama-3-8B-Instruct.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\n",
            "[INFO|configuration_utils.py:955] 2024-07-17 17:14:29,792 >> loading configuration file generation_config.json from cache at /local/data/huggingface_cache/hub/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/e1945c40cd546c78e41f1151f4db032b271faeaa/generation_config.json\n",
            "[INFO|configuration_utils.py:1000] 2024-07-17 17:14:29,792 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"do_sample\": true,\n",
            "  \"eos_token_id\": [\n",
            "    128001,\n",
            "    128009\n",
            "  ],\n",
            "  \"max_length\": 4096,\n",
            "  \"temperature\": 0.6,\n",
            "  \"top_p\": 0.9\n",
            "}\n",
            "\n",
            "07/17/2024 17:14:29 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.\n",
            "07/17/2024 17:14:32 - INFO - llamafactory.model.adapter - Merged 1 adapter(s).\n",
            "07/17/2024 17:14:32 - INFO - llamafactory.model.adapter - Loaded adapter(s): saves/LLaMA3-8B-Chat/lora/sft-hupd\n",
            "07/17/2024 17:14:32 - INFO - llamafactory.model.loader - all params: 8,030,261,248\n",
            "07/17/2024 17:14:32 - WARNING - llamafactory.chat.hf_engine - There is no current event loop, creating a new one.\n",
            "Traceback (most recent call last):\n",
            "  File \"/local/data/hs3447/anaconda3/envs/factory/lib/python3.10/site-packages/transformers/utils/versions.py\", line 102, in require_version\n",
            "    got_ver = importlib.metadata.version(pkg)\n",
            "  File \"/local/data/hs3447/anaconda3/envs/factory/lib/python3.10/importlib/metadata/__init__.py\", line 996, in version\n",
            "    return distribution(distribution_name).version\n",
            "  File \"/local/data/hs3447/anaconda3/envs/factory/lib/python3.10/importlib/metadata/__init__.py\", line 969, in distribution\n",
            "    return Distribution.from_name(distribution_name)\n",
            "  File \"/local/data/hs3447/anaconda3/envs/factory/lib/python3.10/importlib/metadata/__init__.py\", line 548, in from_name\n",
            "    raise PackageNotFoundError(name)\n",
            "importlib.metadata.PackageNotFoundError: No package metadata was found for vllm\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/local/data/hs3447/anaconda3/envs/factory/lib/python3.10/site-packages/gradio/queueing.py\", line 575, in process_events\n",
            "    response = await route_utils.call_process_api(\n",
            "  File \"/local/data/hs3447/anaconda3/envs/factory/lib/python3.10/site-packages/gradio/route_utils.py\", line 276, in call_process_api\n",
            "    output = await app.get_blocks().process_api(\n",
            "  File \"/local/data/hs3447/anaconda3/envs/factory/lib/python3.10/site-packages/gradio/blocks.py\", line 1897, in process_api\n",
            "    result = await self.call_function(\n",
            "  File \"/local/data/hs3447/anaconda3/envs/factory/lib/python3.10/site-packages/gradio/blocks.py\", line 1495, in call_function\n",
            "    prediction = await utils.async_iteration(iterator)\n",
            "  File \"/local/data/hs3447/anaconda3/envs/factory/lib/python3.10/site-packages/gradio/utils.py\", line 661, in async_iteration\n",
            "    return await iterator.__anext__()\n",
            "  File \"/local/data/hs3447/anaconda3/envs/factory/lib/python3.10/site-packages/gradio/utils.py\", line 654, in __anext__\n",
            "    return await anyio.to_thread.run_sync(\n",
            "  File \"/local/data/hs3447/anaconda3/envs/factory/lib/python3.10/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
            "    return await get_async_backend().run_sync_in_worker_thread(\n",
            "  File \"/local/data/hs3447/anaconda3/envs/factory/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 2177, in run_sync_in_worker_thread\n",
            "    return await future\n",
            "  File \"/local/data/hs3447/anaconda3/envs/factory/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 859, in run\n",
            "    result = context.run(func, *args)\n",
            "  File \"/local/data/hs3447/anaconda3/envs/factory/lib/python3.10/site-packages/gradio/utils.py\", line 637, in run_sync_iterator_async\n",
            "    return next(iterator)\n",
            "  File \"/local/data/hs3447/anaconda3/envs/factory/lib/python3.10/site-packages/gradio/utils.py\", line 799, in gen_wrapper\n",
            "    response = next(iterator)\n",
            "  File \"/local/data/hs3447/llama3/LLaMA-Factory/src/llamafactory/webui/chatter.py\", line 107, in load_model\n",
            "    super().__init__(args)\n",
            "  File \"/local/data/hs3447/llama3/LLaMA-Factory/src/llamafactory/chat/chat_model.py\", line 42, in __init__\n",
            "    model_args, data_args, finetuning_args, generating_args = get_infer_args(args)\n",
            "  File \"/local/data/hs3447/llama3/LLaMA-Factory/src/llamafactory/hparams/parser.py\", line 381, in get_infer_args\n",
            "    _check_extra_dependencies(model_args, finetuning_args)\n",
            "  File \"/local/data/hs3447/llama3/LLaMA-Factory/src/llamafactory/hparams/parser.py\", line 123, in _check_extra_dependencies\n",
            "    require_version(\"vllm>=0.4.3\", \"To fix: pip install vllm>=0.4.3\")\n",
            "  File \"/local/data/hs3447/anaconda3/envs/factory/lib/python3.10/site-packages/transformers/utils/versions.py\", line 104, in require_version\n",
            "    raise importlib.metadata.PackageNotFoundError(\n",
            "importlib.metadata.PackageNotFoundError: No package metadata was found for The 'vllm>=0.4.3' distribution was not found and is required by this application. \n",
            "To fix: pip install vllm>=0.4.3\n",
            "Traceback (most recent call last):\n",
            "  File \"/local/data/hs3447/anaconda3/envs/factory/lib/python3.10/site-packages/transformers/utils/versions.py\", line 102, in require_version\n",
            "    got_ver = importlib.metadata.version(pkg)\n",
            "  File \"/local/data/hs3447/anaconda3/envs/factory/lib/python3.10/importlib/metadata/__init__.py\", line 996, in version\n",
            "    return distribution(distribution_name).version\n",
            "  File \"/local/data/hs3447/anaconda3/envs/factory/lib/python3.10/importlib/metadata/__init__.py\", line 969, in distribution\n",
            "    return Distribution.from_name(distribution_name)\n",
            "  File \"/local/data/hs3447/anaconda3/envs/factory/lib/python3.10/importlib/metadata/__init__.py\", line 548, in from_name\n",
            "    raise PackageNotFoundError(name)\n",
            "importlib.metadata.PackageNotFoundError: No package metadata was found for vllm\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/local/data/hs3447/anaconda3/envs/factory/lib/python3.10/site-packages/gradio/queueing.py\", line 575, in process_events\n",
            "    response = await route_utils.call_process_api(\n",
            "  File \"/local/data/hs3447/anaconda3/envs/factory/lib/python3.10/site-packages/gradio/route_utils.py\", line 276, in call_process_api\n",
            "    output = await app.get_blocks().process_api(\n",
            "  File \"/local/data/hs3447/anaconda3/envs/factory/lib/python3.10/site-packages/gradio/blocks.py\", line 1897, in process_api\n",
            "    result = await self.call_function(\n",
            "  File \"/local/data/hs3447/anaconda3/envs/factory/lib/python3.10/site-packages/gradio/blocks.py\", line 1495, in call_function\n",
            "    prediction = await utils.async_iteration(iterator)\n",
            "  File \"/local/data/hs3447/anaconda3/envs/factory/lib/python3.10/site-packages/gradio/utils.py\", line 661, in async_iteration\n",
            "    return await iterator.__anext__()\n",
            "  File \"/local/data/hs3447/anaconda3/envs/factory/lib/python3.10/site-packages/gradio/utils.py\", line 654, in __anext__\n",
            "    return await anyio.to_thread.run_sync(\n",
            "  File \"/local/data/hs3447/anaconda3/envs/factory/lib/python3.10/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
            "    return await get_async_backend().run_sync_in_worker_thread(\n",
            "  File \"/local/data/hs3447/anaconda3/envs/factory/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 2177, in run_sync_in_worker_thread\n",
            "    return await future\n",
            "  File \"/local/data/hs3447/anaconda3/envs/factory/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 859, in run\n",
            "    result = context.run(func, *args)\n",
            "  File \"/local/data/hs3447/anaconda3/envs/factory/lib/python3.10/site-packages/gradio/utils.py\", line 637, in run_sync_iterator_async\n",
            "    return next(iterator)\n",
            "  File \"/local/data/hs3447/anaconda3/envs/factory/lib/python3.10/site-packages/gradio/utils.py\", line 799, in gen_wrapper\n",
            "    response = next(iterator)\n",
            "  File \"/local/data/hs3447/llama3/LLaMA-Factory/src/llamafactory/webui/chatter.py\", line 107, in load_model\n",
            "    super().__init__(args)\n",
            "  File \"/local/data/hs3447/llama3/LLaMA-Factory/src/llamafactory/chat/chat_model.py\", line 42, in __init__\n",
            "    model_args, data_args, finetuning_args, generating_args = get_infer_args(args)\n",
            "  File \"/local/data/hs3447/llama3/LLaMA-Factory/src/llamafactory/hparams/parser.py\", line 381, in get_infer_args\n",
            "    _check_extra_dependencies(model_args, finetuning_args)\n",
            "  File \"/local/data/hs3447/llama3/LLaMA-Factory/src/llamafactory/hparams/parser.py\", line 123, in _check_extra_dependencies\n",
            "    require_version(\"vllm>=0.4.3\", \"To fix: pip install vllm>=0.4.3\")\n",
            "  File \"/local/data/hs3447/anaconda3/envs/factory/lib/python3.10/site-packages/transformers/utils/versions.py\", line 104, in require_version\n",
            "    raise importlib.metadata.PackageNotFoundError(\n",
            "importlib.metadata.PackageNotFoundError: No package metadata was found for The 'vllm>=0.4.3' distribution was not found and is required by this application. \n",
            "To fix: pip install vllm>=0.4.3\n",
            "[INFO|tokenization_utils_base.py:2161] 2024-07-17 17:16:26,814 >> loading file tokenizer.json from cache at /local/data/huggingface_cache/hub/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/e1945c40cd546c78e41f1151f4db032b271faeaa/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2161] 2024-07-17 17:16:26,814 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2161] 2024-07-17 17:16:26,814 >> loading file special_tokens_map.json from cache at /local/data/huggingface_cache/hub/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/e1945c40cd546c78e41f1151f4db032b271faeaa/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2161] 2024-07-17 17:16:26,814 >> loading file tokenizer_config.json from cache at /local/data/huggingface_cache/hub/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/e1945c40cd546c78e41f1151f4db032b271faeaa/tokenizer_config.json\n",
            "[WARNING|logging.py:313] 2024-07-17 17:16:27,103 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "07/17/2024 17:16:27 - INFO - llamafactory.data.template - Replace eos token: <|eot_id|>\n",
            "07/17/2024 17:16:27 - INFO - llamafactory.data.template - Add pad token: <|eot_id|>\n",
            "[INFO|configuration_utils.py:733] 2024-07-17 17:16:27,136 >> loading configuration file config.json from cache at /local/data/huggingface_cache/hub/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/e1945c40cd546c78e41f1151f4db032b271faeaa/config.json\n",
            "[INFO|configuration_utils.py:800] 2024-07-17 17:16:27,136 >> Model config LlamaConfig {\n",
            "  \"_name_or_path\": \"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
            "  \"architectures\": [\n",
            "    \"LlamaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"eos_token_id\": 128009,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 14336,\n",
            "  \"max_position_embeddings\": 8192,\n",
            "  \"mlp_bias\": false,\n",
            "  \"model_type\": \"llama\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"pretraining_tp\": 1,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 500000.0,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.42.4\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 128256\n",
            "}\n",
            "\n",
            "07/17/2024 17:16:27 - INFO - llamafactory.model.patcher - Using KV cache for faster generation.\n",
            "[INFO|modeling_utils.py:3556] 2024-07-17 17:16:27,137 >> loading weights file model.safetensors from cache at /local/data/huggingface_cache/hub/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/e1945c40cd546c78e41f1151f4db032b271faeaa/model.safetensors.index.json\n",
            "[INFO|modeling_utils.py:1531] 2024-07-17 17:16:27,138 >> Instantiating LlamaForCausalLM model under default dtype torch.bfloat16.\n",
            "[INFO|configuration_utils.py:1000] 2024-07-17 17:16:27,139 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"eos_token_id\": 128009\n",
            "}\n",
            "\n",
            "Loading checkpoint shards: 100%|██████████████████| 4/4 [00:08<00:00,  2.09s/it]\n",
            "[INFO|modeling_utils.py:4364] 2024-07-17 17:16:35,561 >> All model checkpoint weights were used when initializing LlamaForCausalLM.\n",
            "\n",
            "[INFO|modeling_utils.py:4372] 2024-07-17 17:16:35,561 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at meta-llama/Meta-Llama-3-8B-Instruct.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\n",
            "[INFO|configuration_utils.py:955] 2024-07-17 17:16:35,593 >> loading configuration file generation_config.json from cache at /local/data/huggingface_cache/hub/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/e1945c40cd546c78e41f1151f4db032b271faeaa/generation_config.json\n",
            "[INFO|configuration_utils.py:1000] 2024-07-17 17:16:35,593 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"do_sample\": true,\n",
            "  \"eos_token_id\": [\n",
            "    128001,\n",
            "    128009\n",
            "  ],\n",
            "  \"max_length\": 4096,\n",
            "  \"temperature\": 0.6,\n",
            "  \"top_p\": 0.9\n",
            "}\n",
            "\n",
            "07/17/2024 17:16:35 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.\n",
            "07/17/2024 17:16:38 - INFO - llamafactory.model.adapter - Merged 1 adapter(s).\n",
            "07/17/2024 17:16:38 - INFO - llamafactory.model.adapter - Loaded adapter(s): saves/LLaMA3-8B-Chat/lora/sft-hupd\n",
            "07/17/2024 17:16:38 - INFO - llamafactory.model.loader - all params: 8,030,261,248\n",
            "07/17/2024 17:16:38 - WARNING - llamafactory.chat.hf_engine - There is no current event loop, creating a new one.\n",
            "^C\n",
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 0.0.0.0:7861 <> https://e1debf16f8fc12fbbc.gradio.live\n"
          ]
        }
      ],
      "source": [
        "!GRADIO_SHARE=1 llamafactory-cli webui"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rgR3UFhB0Ifq"
      },
      "source": [
        "## Fine-tune model via Command Line\n",
        "\n",
        "It takes ~30min for training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CS0Qk5OR0i4Q"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "args = dict(\n",
        "  stage=\"sft\",                        # do supervised fine-tuning\n",
        "  do_train=True,\n",
        "  model_name_or_path=\"unsloth/llama-3-8b-Instruct-bnb-4bit\", # use bnb-4bit-quantized Llama-3-8B-Instruct model\n",
        "  dataset=\"identity,alpaca_en_demo\",             # use alpaca and identity datasets\n",
        "  template=\"llama3\",                     # use llama3 prompt template\n",
        "  finetuning_type=\"lora\",                   # use LoRA adapters to save memory\n",
        "  lora_target=\"all\",                     # attach LoRA adapters to all linear layers\n",
        "  output_dir=\"llama3_lora\",                  # the path to save LoRA adapters\n",
        "  per_device_train_batch_size=2,               # the batch size\n",
        "  gradient_accumulation_steps=4,               # the gradient accumulation steps\n",
        "  lr_scheduler_type=\"cosine\",                 # use cosine learning rate scheduler\n",
        "  logging_steps=10,                      # log every 10 steps\n",
        "  warmup_ratio=0.1,                      # use warmup scheduler\n",
        "  save_steps=1000,                      # save checkpoint every 1000 steps\n",
        "  learning_rate=5e-5,                     # the learning rate\n",
        "  num_train_epochs=3.0,                    # the epochs of training\n",
        "  max_samples=500,                      # use 500 examples in each dataset\n",
        "  max_grad_norm=1.0,                     # clip gradient norm to 1.0\n",
        "  quantization_bit=4,                     # use 4-bit QLoRA\n",
        "  loraplus_lr_ratio=16.0,                   # use LoRA+ algorithm with lambda=16.0\n",
        "  fp16=True,                         # use float16 mixed precision training\n",
        ")\n",
        "\n",
        "json.dump(args, open(\"train_llama3.json\", \"w\", encoding=\"utf-8\"), indent=2)\n",
        "\n",
        "%cd /content/LLaMA-Factory/\n",
        "\n",
        "!llamafactory-cli train train_llama3.json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PVNaC-xS5N40"
      },
      "source": [
        "## Infer the fine-tuned model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oh8H9A_25SF9"
      },
      "outputs": [],
      "source": [
        "from llamafactory.chat import ChatModel\n",
        "from llamafactory.extras.misc import torch_gc\n",
        "\n",
        "%cd /content/LLaMA-Factory/\n",
        "\n",
        "args = dict(\n",
        "  model_name_or_path=\"unsloth/llama-3-8b-Instruct-bnb-4bit\", # use bnb-4bit-quantized Llama-3-8B-Instruct model\n",
        "  adapter_name_or_path=\"llama3_lora\",            # load the saved LoRA adapters\n",
        "  template=\"llama3\",                     # same to the one in training\n",
        "  finetuning_type=\"lora\",                  # same to the one in training\n",
        "  quantization_bit=4,                    # load 4-bit quantized model\n",
        ")\n",
        "chat_model = ChatModel(args)\n",
        "\n",
        "messages = []\n",
        "print(\"Welcome to the CLI application, use `clear` to remove the history, use `exit` to exit the application.\")\n",
        "while True:\n",
        "  query = input(\"\\nUser: \")\n",
        "  if query.strip() == \"exit\":\n",
        "    break\n",
        "  if query.strip() == \"clear\":\n",
        "    messages = []\n",
        "    torch_gc()\n",
        "    print(\"History has been removed.\")\n",
        "    continue\n",
        "\n",
        "  messages.append({\"role\": \"user\", \"content\": query})\n",
        "  print(\"Assistant: \", end=\"\", flush=True)\n",
        "\n",
        "  response = \"\"\n",
        "  for new_text in chat_model.stream_chat(messages):\n",
        "    print(new_text, end=\"\", flush=True)\n",
        "    response += new_text\n",
        "  print()\n",
        "  messages.append({\"role\": \"assistant\", \"content\": response})\n",
        "\n",
        "torch_gc()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kTESHaFvbNTr"
      },
      "source": [
        "## Merge the LoRA adapter and optionally upload model\n",
        "\n",
        "NOTE: the Colab free version has merely 12GB RAM, where merging LoRA of a 8B model needs at least 18GB RAM, thus you **cannot** perform it in the free version."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mcNcHcA4bf4Z"
      },
      "outputs": [],
      "source": [
        "!huggingface-cli login"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IMojogHbaOZF"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "args = dict(\n",
        "  model_name_or_path=\"meta-llama/Meta-Llama-3-8B-Instruct\", # use official non-quantized Llama-3-8B-Instruct model\n",
        "  adapter_name_or_path=\"llama3_lora\",            # load the saved LoRA adapters\n",
        "  template=\"llama3\",                     # same to the one in training\n",
        "  finetuning_type=\"lora\",                  # same to the one in training\n",
        "  export_dir=\"llama3_lora_merged\",              # the path to save the merged model\n",
        "  export_size=2,                       # the file shard size (in GB) of the merged model\n",
        "  export_device=\"cpu\",                    # the device used in export, can be chosen from `cpu` and `cuda`\n",
        "  #export_hub_model_id=\"your_id/your_model\",         # the Hugging Face hub ID to upload model\n",
        ")\n",
        "\n",
        "json.dump(args, open(\"merge_llama3.json\", \"w\", encoding=\"utf-8\"), indent=2)\n",
        "\n",
        "%cd /content/LLaMA-Factory/\n",
        "\n",
        "!llamafactory-cli export merge_llama3.json"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "factory",
      "language": "python",
      "name": "factory"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
